{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "540ceaad-989e-432f-92f5-c13c85af46cb",
   "metadata": {},
   "source": [
    "**Vision Model v0** \\\n",
    "Goal: Recognize images of numbers from a Hugging Face dataset with the appropriate labels\\\n",
    "Author: Atman Singh\\\n",
    "Date: 12/29/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b75a49-acda-4115-83c8-d6a4f299361b",
   "metadata": {},
   "source": [
    "**Data Wrangling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdf5f688-9af7-4d01-82ea-a5d44bb284c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atman\\OneDrive\\Documents\\Repos\\weather-ai-app\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a88e48aa-2928-48bf-9486-8a10c8b2e35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"ylecun/mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fe6df27-3699-4eef-81c3-8762b67043b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 60000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6872d691-71f7-437e-8446-d919d576a7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.cuda\n",
    "import numpy as np\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08936a5d-bff0-4b48-b0a7-8757eb41455b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert pngs to tensors containing pixel values\n",
    "rows = 60000\n",
    "width = 28\n",
    "height = 28\n",
    "\n",
    "images = torch.empty((rows, width, height), dtype=torch.int64)\n",
    "for i, row in enumerate(ds['train'].select(range(rows))):\n",
    "    images[i] = torch.reshape(torch.tensor(list(row['image'].getdata())), (width, height))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac20428-f4e1-4303-a80f-349319442f54",
   "metadata": {},
   "source": [
    "**Forward Propagation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4381078-7668-4c80-9a14-789ede43cf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define kernels\n",
    "torch.manual_seed(3500)\n",
    "min_fence = -0.2\n",
    "max_fence = 0.2\n",
    "\n",
    "# initialize kernels with random values and transform them to range [-0.2, 0.2)\n",
    "kernel_layer_one = torch.rand(2, 1, 5, 5)\n",
    "kernel_layer_one = kernel_layer_one * (max_fence - min_fence) + min_fence\n",
    "\n",
    "kernel_layer_two = torch.rand(4, 2, 3, 3)\n",
    "kernel_layer_two = kernel_layer_two * (max_fence - min_fence) + min_fence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1391225-cf8f-46f5-a785-6199d24567bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.1782,  0.0646,  0.1120, -0.1235, -0.0251],\n",
      "          [-0.0813,  0.0340, -0.0240,  0.0107, -0.1173],\n",
      "          [ 0.1244,  0.0044,  0.1358, -0.0763,  0.0652],\n",
      "          [-0.0817,  0.0238, -0.0186, -0.1438, -0.1802],\n",
      "          [ 0.0320, -0.0673, -0.0626, -0.1702, -0.1149]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1213, -0.1190, -0.0966,  0.0991,  0.1113],\n",
      "          [-0.0201, -0.1052,  0.0012,  0.0649,  0.1585],\n",
      "          [ 0.0298, -0.0734,  0.1919,  0.1951, -0.0007],\n",
      "          [ 0.0821,  0.1832, -0.0529, -0.0707, -0.1110],\n",
      "          [ 0.1072,  0.1901,  0.0652, -0.0375,  0.0644]]]])\n",
      "tensor([[[[-0.1835,  0.0642,  0.1907],\n",
      "          [ 0.0446,  0.0712,  0.1137],\n",
      "          [ 0.1337,  0.0457, -0.0508]],\n",
      "\n",
      "         [[-0.1543,  0.0988,  0.0077],\n",
      "          [ 0.1449, -0.0093,  0.0008],\n",
      "          [ 0.0122,  0.1464,  0.0089]]],\n",
      "\n",
      "\n",
      "        [[[-0.1186,  0.0576,  0.0205],\n",
      "          [ 0.1428, -0.1741,  0.1910],\n",
      "          [-0.0419,  0.0147,  0.0765]],\n",
      "\n",
      "         [[ 0.0232, -0.1500,  0.1792],\n",
      "          [-0.1863, -0.0985, -0.1176],\n",
      "          [-0.1673,  0.1240,  0.0591]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1024,  0.1868, -0.0118],\n",
      "          [ 0.1951,  0.0451, -0.0717],\n",
      "          [ 0.0556, -0.0477,  0.0019]],\n",
      "\n",
      "         [[-0.1987, -0.0412,  0.1624],\n",
      "          [ 0.0433,  0.0925,  0.0322],\n",
      "          [-0.0609, -0.1494,  0.0972]]],\n",
      "\n",
      "\n",
      "        [[[-0.1164,  0.0035, -0.0159],\n",
      "          [-0.0186, -0.1291,  0.0880],\n",
      "          [ 0.0952,  0.0840,  0.1262]],\n",
      "\n",
      "         [[ 0.1558,  0.0044, -0.1837],\n",
      "          [ 0.0335,  0.0497, -0.1027],\n",
      "          [-0.1517,  0.0178, -0.1791]]]])\n"
     ]
    }
   ],
   "source": [
    "print(kernel_layer_one)\n",
    "print(kernel_layer_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5206aa57-a329-4208-99aa-03ac3d03641f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Transforms matrices into vectors, verifies than they are of the same length, \n",
    "then performs a convolution on the vectors by multiplying the corresponding \n",
    "components and taking the sum of those products\n",
    "\"\"\"\n",
    "def convolve(m1: torch.Tensor, m2: torch.Tensor) -> torch.Tensor:\n",
    "    if len(m1.shape) != 2 or len(m2.shape) != 2:\n",
    "        raise Exception(f\"Matrix m1 has a rank of {len(m1.shape)} and \" + \n",
    "                        f\"Matrix m2 has a rank of {len(m2.shape)}\")\n",
    "    if m1.shape[0] * m1.shape[1] != m2.shape[0] * m2.shape[1]:\n",
    "        raise Exception(\"Linear length of matrices are not equal\")\n",
    "    m1 = torch.reshape(m1, (-1,)) \n",
    "    m2 = torch.reshape(m2, (-1,))\n",
    "    return sum(m1 * m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63df77fb-c175-4521-85d6-364e5840e94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "fc9f95ac-436d-4bfd-91a3-27e467b76dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse_image(image: torch.Tensor, kernel: torch.Tensor, step: int) -> torch.Tensor:\n",
    "    if len(kernel.shape) != 2:\n",
    "        raise Exception(f\"Kernel has a rank of {len(kernel.shape)}\")\n",
    "    if kernel.shape[0] % 2 == 0:\n",
    "        raise Exception(\"Kernel matrix rank is not odd\")\n",
    "        \n",
    "    width = kernel.shape[0]\n",
    "    output = []\n",
    "    \n",
    "    for i in range(0, image.shape[1] - width + 1, step):\n",
    "        for j in range(0, image.shape[0] - width + 1, step):\n",
    "            current = image[i:i+width,j:j+width]\n",
    "            convolution = convolve(current, kernel)\n",
    "            output.append(convolution)\n",
    "                \n",
    "    output = torch.Tensor(output)\n",
    "    rank = int(math.sqrt(len(output)))\n",
    "    output = torch.reshape(output, (rank, rank))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "8ca7c821-2bd0-4424-8d05-24cba3288633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pool(matrix: torch.Tensor, pool_size: tuple) -> torch.Tensor:\n",
    "    if len(matrix.shape) != 2:\n",
    "        raise Exception(f\"Matrix has a rank of {len(matrix.shape)}\")\n",
    "    if matrix.shape[0] % pool_size[0] != 0 or matrix.shape[1] % pool_size[1] != 0:\n",
    "        raise Exception(f\"Pool size {pool_size} is not a multiple of matrix shape {matrix.shape}\")\n",
    "\n",
    "    width = pool_size[1]\n",
    "    height = pool_size[0]\n",
    "    output = []\n",
    "    \n",
    "    for i in range(0, matrix.shape[1] - width + 1, width):\n",
    "        for j in range(0, matrix.shape[0] - height + 1, height):\n",
    "            output.append(torch.max(matrix[i:i+width,j:j+height]))\n",
    "    \n",
    "    output = torch.Tensor(output)\n",
    "    rank = int(math.sqrt(len(output)))\n",
    "    output = torch.reshape(output, (rank, rank))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "bd7b72ed-9e7b-412c-9220-a5baf1be1a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(data: torch.Tensor, kernels: list, activation_functions: list, step: int, pool_size: tuple) -> torch.Tensor:\n",
    "    CHANNEL_INDEX = 1\n",
    "    MS_PER_MATRIX = 568.25\n",
    "    print(f\"ETA: {round(len(data) * (MS_PER_MATRIX / 1000 / 60), 2)} minutes\")\n",
    "    \n",
    "    if not (isinstance(data, torch.Tensor) or isinstance(data, list)):\n",
    "        raise Exception(f\"Data is not of type 'list', inputted type is: {type(data)}\")\n",
    "    if not isinstance(kernels, list):\n",
    "        raise Exception(f\"Kernels is not of type 'list', inputted type is: {type(kernels)}\")\n",
    "    if len(kernels) != len(activation_functions):\n",
    "        raise Exception(f\"Number of kernels inputted ({len(kernels)}) does \" +\n",
    "                        f\"not equal number of activation functions inputted \" +\n",
    "                        f\"({len(activationf)})\")\n",
    "\n",
    "    output = []\n",
    "    for i, matrix in enumerate(data):\n",
    "        current_matrices = [matrix]\n",
    "        for j, kernel_layer in enumerate(kernels):\n",
    "            if kernel_layer.shape[CHANNEL_INDEX] != len(current_matrices):\n",
    "                    raise Exception(f\"On iteration {j}: Number of kernels ({kernel_layer.shape[CHANNEL_INDEX]}) \" +\n",
    "                                    f\"does not equal number of channels ({len(current_matrices)})\")\n",
    "            if activation_functions[j].lower() == 'relu':\n",
    "                func = torch.nn.ReLU()\n",
    "            elif activation_functions[j].lower() == 'sigmoid':\n",
    "                func = torch.nn.Sigmoid()\n",
    "            else:\n",
    "                raise Exception(f'Activation function \"{activation_functions[j]}\" ' +\n",
    "                               f\"is not a valid activation function.\")\n",
    "                \n",
    "            updated_matrices = []\n",
    "            for kernel in kernel_layer:   \n",
    "                convolutions = []\n",
    "                for current_matrix, channel in zip(current_matrices, kernel):\n",
    "                    channel_convolution = traverse_image(current_matrix, channel, step)\n",
    "                    convolutions.append(channel_convolution)\n",
    "                \n",
    "                convolution = func(torch.stack(convolutions, dim=0).sum(dim=0))\n",
    "                convolution_pooled = max_pool(convolution, pool_size)\n",
    "                updated_matrices.append(convolution_pooled)\n",
    "                \n",
    "            current_matrices = updated_matrices\n",
    "            \n",
    "        current_matrices = torch.stack(current_matrices, dim = 0)\n",
    "        output.append(current_matrices)\n",
    "\n",
    "    return torch.stack(output, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "20099b6b-dc17-471f-9e83-950bd52f911d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETA: 0.01 minutes\n",
      "tensor([[[[1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00],\n",
      "          [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.5345e-01],\n",
      "          [9.9962e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00],\n",
      "          [9.9939e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00],\n",
      "          [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.5363e-01]],\n",
      "\n",
      "         [[9.9998e-01, 9.9960e-01, 1.5647e-12, 2.4972e-21, 6.5391e-18],\n",
      "          [9.9996e-01, 9.5045e-01, 5.8247e-12, 1.1468e-10, 2.9252e-01],\n",
      "          [1.0000e+00, 9.5221e-01, 9.3785e-01, 9.0696e-08, 4.3083e-12],\n",
      "          [9.9999e-01, 1.0000e+00, 9.9989e-01, 5.3226e-19, 1.1943e-04],\n",
      "          [9.9999e-01, 7.8089e-16, 4.1102e-18, 8.9185e-01, 2.1627e-01]],\n",
      "\n",
      "         [[1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9761e-01, 9.6000e-01],\n",
      "          [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0042e-08, 1.1676e-02],\n",
      "          [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.3398e-03, 5.3263e-03],\n",
      "          [9.9996e-01, 1.0000e+00, 1.0000e+00, 2.1976e-03, 6.8715e-10],\n",
      "          [1.0000e+00, 1.0000e+00, 1.0000e+00, 5.3075e-06, 7.9571e-01]],\n",
      "\n",
      "         [[4.0027e-03, 1.5602e-18, 1.3824e-30, 1.7255e-07, 8.1300e-01],\n",
      "          [1.8122e-13, 2.2944e-12, 3.7750e-01, 4.2031e-01, 1.0000e+00],\n",
      "          [4.2890e-01, 1.4325e-08, 1.4329e-13, 1.0342e-06, 9.9939e-01],\n",
      "          [3.5287e-01, 9.9087e-11, 1.5861e-24, 2.2526e-06, 1.0000e+00],\n",
      "          [1.1658e-15, 6.4770e-18, 1.0000e+00, 1.0000e+00, 1.0000e+00]]]])\n",
      "CPU times: total: 297 ms\n",
      "Wall time: 484 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# feature extraction using forward propagation pipeline\n",
    "kernels = [kernel_layer_one, kernel_layer_two]\n",
    "activation_functions = ['relu', 'sigmoid']\n",
    "features = forward(images[:1], kernels, activation_functions, 1, (2,2))\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f789fd60-95dd-4977-9900-8268eef9108b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
      "        1.0000e+00, 1.0000e+00, 1.0000e+00, 9.5345e-01, 9.9962e-01, 1.0000e+00,\n",
      "        1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9939e-01, 1.0000e+00, 1.0000e+00,\n",
      "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
      "        9.5363e-01, 9.9998e-01, 9.9960e-01, 1.5647e-12, 2.4972e-21, 6.5391e-18,\n",
      "        9.9996e-01, 9.5045e-01, 5.8247e-12, 1.1468e-10, 2.9252e-01, 1.0000e+00,\n",
      "        9.5221e-01, 9.3785e-01, 9.0696e-08, 4.3083e-12, 9.9999e-01, 1.0000e+00,\n",
      "        9.9989e-01, 5.3226e-19, 1.1943e-04, 9.9999e-01, 7.8089e-16, 4.1102e-18,\n",
      "        8.9185e-01, 2.1627e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9761e-01,\n",
      "        9.6000e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0042e-08, 1.1676e-02,\n",
      "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.3398e-03, 5.3263e-03, 9.9996e-01,\n",
      "        1.0000e+00, 1.0000e+00, 2.1976e-03, 6.8715e-10, 1.0000e+00, 1.0000e+00,\n",
      "        1.0000e+00, 5.3075e-06, 7.9571e-01, 4.0027e-03, 1.5602e-18, 1.3824e-30,\n",
      "        1.7255e-07, 8.1300e-01, 1.8122e-13, 2.2944e-12, 3.7750e-01, 4.2031e-01,\n",
      "        1.0000e+00, 4.2890e-01, 1.4325e-08, 1.4329e-13, 1.0342e-06, 9.9939e-01,\n",
      "        3.5287e-01, 9.9087e-11, 1.5861e-24, 2.2526e-06, 1.0000e+00, 1.1658e-15,\n",
      "        6.4770e-18, 1.0000e+00, 1.0000e+00, 1.0000e+00])\n"
     ]
    }
   ],
   "source": [
    "# flattening\n",
    "flattened = torch.flatten(features)\n",
    "print(flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8170e25-2ab5-4500-854a-6f36ea7d9a0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
